ML

1. Uber 

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv("uber.csv")

df.head

df.describe()

df.info()

df["pickup_datetime"] = pd.to_datetime(df["pickup_datetime"])

df.isnull().sum()

df.dropna(inplace=True)

df.isnull().sum()

plt.boxplot(df['fare_amount'])

#Remove Outliers
q_low = df["fare_amount"].quantile(0.01)
q_hi  = df["fare_amount"].quantile(0.99)
df = df[(df["fare_amount"] < q_hi) & (df["fare_amount"] > q_low)]

from sklearn.model_selection import train_test_split

#Take x as predictor variable
x = df.drop([df.columns[0],"key","fare_amount"], axis = 1)
#And y as target variable  
y = df['fare_amount']

#Necessary to apply model
x['pickup_datetime'] = pd.to_numeric(pd.to_datetime(x['pickup_datetime']))

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)

from sklearn.linear_model import LinearRegression

lrmodel = LinearRegression()
lrmodel.fit(x_train, y_train)

predict = lrmodel.predict(x_test)

#Check Error
from sklearn.metrics import mean_squared_error, r2_score
lrmodelrmse = np.sqrt(mean_squared_error(predict, y_test))
print("RMSE error for the model is ", lrmodelrmse)
rint("R2 score for the model is ", r2_score(y_test, predict))

#Let's Apply Random Forest Regressor estimator=no. of trees random_state=bootstrapping
from sklearn.ensemble import RandomForestRegressor
rfrmodel = RandomForestRegressor(n_estimators = 100, random_state = 101)

#Fit the Forest
rfrmodel.fit(x_train, y_train)
rfrmodel_pred = rfrmodel.predict(x_test)

#Errors for the forest (High R2 good)
rfrmodel_rmse = np.sqrt(mean_squared_error(y_test, rfrmodel_pred))
print("RMSE value for Random Forest is:",rfrmodel_rmse)
print("R2 score for the model is ", r2_score(y_test, rfrmodel_pred))

---------------------------------------------------------------------------
2. Email

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier

df = pd.read_csv("./emails.csv")

df.head()

df.isnull().sum()

X = df.iloc[:,1:3001]
X

Y = df.iloc[:,-1].values
Y

train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size = 0.25)

svc = SVC(C=1.0,kernel='rbf',gamma='auto')         
# C here is the regularization parameter. Here, L2 penalty is used(default). It is the inverse of the strength of regularization.
# As C increases, model overfits.
# Kernel here is the radial basis function kernel.
# gamma (only used for rbf kernel) : As gamma increases, model overfits.
svc.fit(train_x,train_y)
y_pred2 = svc.predict(test_x)
print("Accuracy Score for SVC : ", accuracy_score(y_pred2,test_y))

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)

knn = KNeighborsClassifier(n_neighbors=7)

knn.fit(X_train, y_train)

print(knn.predict(X_test))

print(knn.score(X_test, y_test))
-------------------------------------------------------------------------

3. Neural Networks

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
sns.set()

dataset = pd.read_csv('Churn_Modelling.csv', index_col = 'RowNumber')
dataset.head()

#Customer ID and Surname would not be relevant as features
X_columns = dataset.columns.tolist()[2:12]
Y_columns = dataset.columns.tolist()[-1:]
print(X_columns)
print(Y_columns)

X = dataset[X_columns].values 
Y = dataset[Y_columns].values

#We need to encode categorical variables such as geography and gender
from sklearn.preprocessing import LabelEncoder
X_column_transformer = LabelEncoder()
X[:, 1] = X_column_transformer.fit_transform(X[:, 1])

#Lets Encode gender now
X[:, 2] = X_column_transformer.fit_transform(X[:, 2])

# Previous encoding result are comparable
# Hot Encoding ensures that machine learning does not assume that higher numbers are more important.
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

pipeline = Pipeline(
    [
        ('Categorizer', ColumnTransformer(
            [
                ("Gender Label Encoder", OneHotEncoder(categories = 'auto', drop = 'first'), [2]),
                ("Geography Label Encoder", OneHotEncoder(categories = 'auto', drop = 'first'), [1])
            ], 
            remainder = 'passthrough', n_jobs = 1)),
        ('Normalizer', StandardScaler())
    ]
)

#Standardize the features
X = pipeline.fit_transform(X)

#Spilt the data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)

#Let us create the Neural Network
from keras.models import Sequential
from keras.layers import Dense, Dropout

#Initialize ANN
classifier = Sequential()

# ReLU = Rectified Linear Unit 
#Add input layer and hidden layer
classifier.add(Dense(6, activation = 'relu', input_shape = (X_train.shape[1], )))
classifier.add(Dropout(rate = 0.1))

#Add second layer
classifier.add(Dense(6, activation = 'relu'))
classifier.add(Dropout(rate = 0.1))

#Add output layer
classifier.add(Dense(1, activation = 'sigmoid'))

#Let us take a look at our network
classifier.summary()

#Optimize the weights
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

#Fitting the Neural Network
history = classifier.fit(X_train, y_train, batch_size = 32, epochs = 200, validation_split = 0.1, verbose = 2)

y_pred = classifier.predict(X_test)
print(y_pred[:5])

#Let us use confusion matrix with cutoff value as 0.5
y_pred = (y_pred > 0.5).astype(int)
print(y_pred[:5])

#Making the Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

#Accuracy of our NN
print(((cm[0][0] + cm[1][1])* 100) / len(y_test), '% of data was classified correctly')

-----------------------------------------------------------------------------------------------

4. Gradient 
#Initialize Parameters
cur_x = 2
rate = 0.01
precision = 0.00000001
previous_step_size = 1
max_iters = 10000
iters = 0
df = lambda x : 2 * (x + 3) #Gradient of our function i.e (x + 3)Â²
#2 * (x+3) is the derivate of the function

#Run a loop to perform gradient Descent
while previous_step_size > precision and iters < max_iters:
    prev_x = cur_x
    cur_x -= rate * df(prev_x)
    previous_step_size = abs(prev_x - cur_x)
    iters += 1
print("Local Minima Occurs at :",cur_x)

# If a function has multiple local minimums and a global minimum, 
it is not guaranteed that gradient descent will find the global minimum.
# NewValue = OldValue - StepSize
# StepSize = LearningRate*Slope 
# Guess a value and then based on the slope decide whether to move ahead or back
# keep doing the process untill the difference between curr and prev is very small

import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(-5,5,100)
y = (x + 3) ** 2

plt.plot(x,y, 'g')
plt.show()

-----------------------------------------------------------------------------------------------

5. KNN on Diabetes

import numpy as np
import pandas as pd

data = pd.read_csv('./diabetes.csv')
data.head()

#Check for null or missing values
data.isnull().sum()

#Replace zero values with mean values
for column in data.columns[1:-3]:
    data[column].replace(0, np.NaN, inplace = True)
    data[column].fillna(round(data[column].mean(skipna=True)), inplace = True)
data.head(10)

X = data.iloc[:, :8] #Features
Y = data.iloc[:, 8:] #Predictor

#Perform Spliting
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

#KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn_fit = knn.fit(X_train, Y_train.values.ravel())
knn_pred = knn_fit.predict(X_test)

from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
print("Confusion Matrix")
print(confusion_matrix(Y_test, knn_pred))
print("Accuracy Score:", accuracy_score(knn_pred, Y_test))
print("Reacal Score:", recall_score(Y_test, knn_pred))
print("F1 Score:", f1_score(Y_test, knn_pred))
print("Precision Score:",precision_score(Y_test, knn_pred))


-------------OR--------------


import numpy as np
import pandas as pd 
import math
from scipy.spatial import distance
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
DataSet = pd.DataFrame()
DataSet = pd.read_csv("diabetes.csv")
DataSet.head(10)
for column in DataSet.columns[1:-3]:
    DataSet[column].replace(0,np.NaN, inplace=True)
    DataSet[column].fillna(round(DataSet[column].mean(skipna=True)), inplace=True)
DataSet.head(10)
X = DataSet.iloc[:, :8]
y = DataSet.iloc[:, 8:]
DataSet.head()
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)
def distance_with_all_train_Points(X_train, X_test):
    eculidean_distance = {}
    for data_point in X_test.itertuples():
        for point in X_train.itertuples():
            eculidean_distance[tuple([list(data_point)[0],list(point)[0]])] = distance.euclidean(list(data_point)[1:], list(point)[1:])
    return eculidean_distance
def k_nearest_neighbors(eculidean_distance, X_test, y, k):
        all_neighbours = {}
        Output_labels = []
        for data_point in X_test.itertuples():
            for key, value in eculidean_distance.items():
                if key[0] == list(data_point)[0]:
                    all_neighbours[key] = value
                else:
                    continue
            i, yes, no = 0, 0, 0
            for item in sorted(all_neighbours.items(), key = lambda x: x[1]):
                if i < k:
                    if(y.iloc[item[0][1]]["Outcome"] == 1):
                        yes += 1
                    else:
                        no += 1
                    i += 1
            if(yes > no):
                Output_labels.append(1)
            else:
                Output_labels.append(0)
            all_neighbours.clear()            
        return Output_labels
eculidean_distance = distance_with_all_train_Points(X_train, X_test)
Output_labels = k_nearest_neighbors(eculidean_distance, X_test, y, 5)
neighbor = KNeighborsClassifier(n_neighbors=5)
neighbor.fit(X_train, np.array(y_train).ravel())
y_pred = neighbor.predict(X_test)
y_pred
neighbor.score(X_test, Output_labels)
from sklearn.metrics import recall_score
recall_score(y_test, y_pred, average=None)
from sklearn.metrics import f1_score
f1_score(y_test, y_pred, average=None)
from sklearn.metrics import precision_score
precision_score(y_test, y_pred, average=None)

---------------------------------------------------------------------------------------------
6. KMeans on Sales

import pandas as pd
import numpy as np

df = pd.read_csv('./sales_data_sample.csv', encoding='unicode_escape')

df.head

df.info

#Columns to Remove
to_drop = ['ADDRESSLINE1', 'ADDRESSLINE2', 'STATE', 'POSTALCODE', 'PHONE']
df = df.drop(to_drop, axis=1)

#Check for null values
df.isnull().sum()

#Bhai bhai look at territory
#But territory does not have significant impact on analysis, let it be 

df.dtypes

#ORDERDATE Should be in date time
df['ORDERDATE'] = pd.to_datetime(df['ORDERDATE'])

#We need to create some features in order to create cluseters
#Recency: Number of days between customer's latest order and today's date
#Frequency : Number of purchases by the customers
#MonetaryValue : Revenue generated by the customers
import datetime as dt
snapshot_date = df['ORDERDATE'].max() + dt.timedelta(days = 1)
df_RFM = df.groupby(['CUSTOMERNAME']).agg({
    'ORDERDATE' : lambda x : (snapshot_date - x.max()).days,
    'ORDERNUMBER' : 'count',
    'SALES' : 'sum'
})
#Rename the columns
df_RFM.rename(columns = {
    'ORDERDATE' : 'Recency',
    'ORDERNUMBER' : 'Frequency',
    'SALES' : 'MonetaryValue'
}, inplace=True)

df_RFM.head()

# Divide into segments
# We create 4 quartile ranges
df_RFM['M'] = pd.qcut(df_RFM['MonetaryValue'], q = 4, labels = range(1,5))
df_RFM['R'] = pd.qcut(df_RFM['Recency'], q = 4, labels = list(range(4,0,-1)))
df_RFM['F'] = pd.qcut(df_RFM['Frequency'], q = 4, labels = range(1,5))
df_RFM.head()

#Create another column for RFM score
df_RFM['RFM_Score'] = df_RFM[['R', 'M', 'F']].sum(axis=1)
df_RFM.head()

def rfm_level(df):
    if bool(df['RFM_Score'] >= 10):
        return 'High Value Customer'
    
    elif bool(df['RFM_Score'] < 10) and bool(df['RFM_Score'] >= 6):
        return 'Mid Value Customer'
    else:
        return 'Low Value Customer'
df_RFM['RFM_Level'] = df_RFM.apply(rfm_level, axis = 1)
df_RFM.head()

# Time to perform KMeans
data = df_RFM[['Recency', 'Frequency', 'MonetaryValue']]
data.head()

# Our data is skewed we must remove it by performing log transformation
data_log = np.log(data)
data_log.head()

#Standardization 
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(data_log)
data_normalized = scaler.transform(data_log)
data_normalized = pd.DataFrame(data_normalized, index = data_log.index, columns=data_log.columns)
data_normalized.describe().round(2)

#Fit KMeans and use elbow method to choose the number of clusters
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
sse = {}
for k in range(1, 21):
    kmeans = KMeans(n_clusters = k, random_state = 1)
    kmeans.fit(data_normalized)
    sse[k] = kmeans.inertia_

plt.figure(figsize=(10,6))
plt.title('The Elbow Method')
plt.xlabel('K')
plt.ylabel('SSE')
plt.style.use('ggplot')
sns.pointplot(x=list(sse.keys()), y = list(sse.values()))
plt.text(4.5, 60, "Largest Angle", bbox = dict(facecolor = 'lightgreen', alpha = 0.5))
plt.show()

# 5 number of clusters seems good
kmeans = KMeans(n_clusters=5, random_state=1)
kmeans.fit(data_normalized)
cluster_labels = kmeans.labels_
data_rfm = data.assign(Cluster = cluster_labels)
data_rfm.head()

-----------------------------------------------------------------------------------------

7. Titanic

import numpy as np 
import pandas as pd
import seaborn as sns
sns.set(palette='colorblind')
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline 
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from sklearn.compose import ColumnTransformer
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
from sklearn.model_selection import cross_val_predict, RandomizedSearchCV, GridSearchCV

raw_data = pd.read_csv('titanic.csv')
raw_data.head()

raw_data.info()

# Checkpoint
data = raw_data.copy()
# Dropping Name and Cabin
data.drop(['Name', 'Cabin'], axis=1, inplace=True)
# Column to change list
column_to_change = ['Pclass', 'SibSp', 'Parch']
data[column_to_change] = data[column_to_change].apply(lambda x: x.astype('category'))
# Instantiate LabelEncoder
lb_encode = LabelEncoder()
data['Ticket'] = lb_encode.fit_transform(data['Ticket'])
# Set Passenger_id as index
data = data.set_index('PassengerId')
data.head()

def validation_fun(df):
    df.drop(['Name', 'Cabin'], axis=1, inplace=True)
    # Column to change list
    column_to_change = ['Pclass', 'SibSp', 'Parch']
    df[column_to_change] = df[column_to_change].apply(lambda x: x.astype('category'))
    # Instantiate LabelEncoder
    lb_encode = LabelEncoder()
    df['Ticket'] = lb_encode.fit_transform(df['Ticket'])
    # Set Passenger_id as index
    df = df.set_index('PassengerId')
    return df

# Split data into inputs and targets
targets = data['Survived']
inputs = data.drop('Survived', axis=1)

cat_names = inputs.select_dtypes(exclude='number').columns
num_names = inputs.select_dtypes(include='number').columns

cat_pipeline = Pipeline(
                        [
                            ('cat_imputer', SimpleImputer(strategy='most_frequent')),
                            ('encoder', OneHotEncoder())
                        ]
)
num_pipeline = Pipeline(
                        [
                            ('num_imputer', SimpleImputer(strategy='median')),
                            ('scaler', StandardScaler())
                        ]
)
full_pipeline = ColumnTransformer(
                                  [
                                      ('cat_pipeline', cat_pipeline, cat_names),
                                      ('num_pipeline', num_pipeline, num_names)
                                  ]
)
pca = PCA()
inputs_scaled = full_pipeline.fit_transform(inputs)

# Instantiate LogisticRegression
lg_reg = LogisticRegression()
lg_reg.fit(inputs_scaled, targets)
baseline_pred = lg_reg.predict(inputs_scaled)
baseline_accuracy = accuracy_score(targets, baseline_pred)
baseline_precision = precision_score(targets, baseline_pred)
baseline_recall = recall_score(targets, baseline_pred)
print(f'The baseline accuracy is {round(baseline_accuracy, 2)*100}%')
print(f'The baseline precision is {round(baseline_precision, 2)*100}%')
print(f'The baseline recall is {round(baseline_recall, 2)*100}%')

# Instantiate XGBClassifier
xgb_class = xgb.XGBClassifier(n_estimators=50, seed=42, objective='reg:logistic')
xgb_class.fit(inputs_scaled, targets)
xgb_pred = xgb_class.predict(inputs_scaled)
xgb_accuracy = accuracy_score(targets, xgb_pred)
xgb_precision = precision_score(targets, xgb_pred)
xgb_recall = recall_score(targets, xgb_pred)
print(f'The XGBClassifier accuracy is {round(xgb_accuracy, 2)*100}%')
print(f'The XGBClassifier precision is {round(xgb_precision, 2)*100}%')
print(f'The XGBClassifier recall is {round(xgb_recall, 2)*100}%')

# Instantiate RandomForestClassifier
rf_class = RandomForestClassifier(n_estimators=200, random_state=42)
rf_class.fit(inputs_scaled, targets)
rf_pred = rf_class.predict(inputs_scaled)
rf_accuracy = accuracy_score(targets, rf_pred)
rf_precision = precision_score(targets, rf_pred)
rf_recall = recall_score(targets, rf_pred)
print(f'The RandomForestClassifier accuracy is {round(rf_accuracy, 2)*100}%')
print(f'The RandomForestClassifier precision is {round(rf_precision, 2)*100}%')
print(f'The RandomForestClassifier recall is {round(rf_recall, 2)*100}%')


# Instantiate GradientBoostingClassifier
gb_class = GradientBoostingClassifier(n_estimators=200, random_state=42)
gb_class.fit(inputs_scaled, targets)
gb_pred = gb_class.predict(inputs_scaled)
gb_accuracy = accuracy_score(targets, gb_pred)
gb_precision = precision_score(targets, gb_pred)
gb_recall = recall_score(targets, gb_pred)
print(f'The GradientBoostingClassifier accuracy is {round(gb_accuracy, 2)*100}%')
print(f'The GradientBoostingClassifier precision is {round(gb_precision, 2)*100}%')
print(f'The GradientBoostingClassifier recall is {round(gb_recall, 2)*100}%')

# Hyper parameter tunning of GradientBoostingClassifier
params = {
    "n_estimators":[5,50,250,500],
    "max_depth":[1,3,5,7,9],
    "learning_rate":[0.01,0.1,1,10,100]
}
gb_cv = RandomizedSearchCV(estimator=gb_class, param_distributions=params,scoring='accuracy', cv=5)
gb_cv.fit(inputs_scaled, targets)
print(f'The best accuracy of GradientBoostingClassifier is {gb_cv.best_score_}')

params = {
    "n_estimators":[64, 128, 256],
    "max_depth":[2, 8, 16]
}
rf_cv = GridSearchCV(estimator=rf_class, param_grid=params,
                               scoring='accuracy', cv=5)
rf_cv.fit(inputs_scaled, targets)
print(f'The best accuracy of RandomForestClassifier is {rf_cv.best_score_}')

# Hyper-parameter tunning of XGBClassifier
params = {
    "learning_rate": np.arange(0.05, 1.05, 0.05),
    "subsample": np.arange(0.05, 1.05, 0.05),
    "n_estimators": [100,200,300,400,500]
}
xgb_cv = RandomizedSearchCV(estimator=xgb_class, param_distributions=params, n_iter=25,
                               scoring='accuracy', cv=5, verbose=1)
xgb_cv.fit(inputs_scaled, targets)
print(f'The best accuracy of XGBClassifier is {xgb_cv.best_score_}')

cv_pred = cross_val_predict(xgb_cv.best_estimator_, inputs_scaled, targets)
cv_accuracy = accuracy_score(targets, cv_pred)
cv_precision = precision_score(targets, cv_pred)
cv_recall = recall_score(targets, cv_pred)
print(f'The Evaluation accuracy is {round(cv_accuracy, 2)*100}%')
print(f'The Evaluation precision is {round(cv_precision, 2)*100}%')
print(f'The Evaluation recall is {round(cv_recall, 2)*100}%')

# Load the test Data 
test_set = pd.read_csv('titanic.csv')
test_set['Parch'] = np.where(test_set['Parch']==9, np.nan, test_set['Parch'])
test_set_data = validation_fun(test_set)
test_scaled = full_pipeline.transform(test_set_data)

best_model = xgb_cv.best_estimator_

test_pred = xgb_class.predict(test_scaled)
submission_df = pd.DataFrame()
submission_df['PassengerId'] = test_set_data.index
submission_df['Survived'] = test_pred
submission_df.set_index('PassengerId', inplace=True)
submission_df.head()

submission_df.to_csv('submission.csv')

---------------------------------------------------------------------------------------------------

BT
1. Bank

pragma solidity 0.4.25;


contract Bank {


    int bal;


    constructor() public {
        bal = 0;
    }


    function getBalance() public view returns(int) {
        return bal;
    }


    function withdraw(int amt) public {
        require(amt > 0, "Invalid Input");
        require(amt < bal, "Insufficient balance");
        bal = bal - amt;
    }


    function deposit(int amt) public {
        require(amt > 0, "Invalid Input");
        bal = bal + amt;
    }
}

---------------------------------------------------------------

2. Election

// SPDX-License-Identifier: MIT


pragma solidity ^0.5.16;
contract Election{


    struct Candidate {
        uint id;
        string name;
        uint voteCount;
    }


    mapping (address => bool) public voters;


    mapping(uint => Candidate) public candidates;


    uint public candidatesCount;
    event votedEvent (
        uint indexed candidateId
    );


    constructor() public {
        addCandidate("Candidate 1");
        addCandidate("Candidate 2");
    }


    function addCandidate(string memory _name) private {
    candidatesCount++;
    candidates[candidatesCount] = Candidate(candidatesCount, _name, 0);
    }
    function vote (uint _candidateId) public {
    require(!voters[msg.sender]);


    require(_candidateId > 0 && _candidateId <= candidatesCount);


    voters[msg.sender] = true;


    candidates[_candidateId].voteCount++;


    emit votedEvent(_candidateId);
    }
}

-----------------------------------------------------------------------

3. Student

// SPDX-License-Identifier: MIT
pragma solidity >= 0.7.0;


contract Student_management{


        struct Student{
                int stud_id;
                string Name;
                string Department;
        }


        Student[] Students;


        function add_stud(int stud_id, string memory Name, string memory Department) public{
                Student memory stud = Student(stud_id, Name, Department);
                Students.push(stud);
        }


        function getStudent(int stud_id) public view returns(string memory, string memory){
                for(uint i = 0; i < Students.length; i++){
                        Student memory stud = Students[i];
                        if(stud.stud_id == stud_id){
                                return(stud.Name, stud.Department);
                        }
                }
        return("Name Not Found", "Department Not Found");
        }


        //Fallback Function
        fallback() external payable{
                Students.push(Student(7, "XYZ", "Mechanical"));
        }
}